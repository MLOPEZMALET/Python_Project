{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import instruments\n",
    "import re\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "##generelise here\n",
    "with open('test.txt','r',encoding='utf-8') as corpus:\n",
    "    co=corpus.readlines()\n",
    "    dic=[]\n",
    "    for e in co:\n",
    "        e.split('\\n')\n",
    "        dic.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic2=[]\n",
    "for e in co:\n",
    "    for i in e.replace('\\n','').replace('?','').replace('!','').replace('.','').replace('’','').replace(',','').split():\n",
    "        dic2.append(i)\n",
    "#si on est interessé au niveau des phrases dic - sans tokenization, dicc avec tokenization\n",
    "dicc=[[e.replace('\\n','').split()]for e in dic]\n",
    "dic3=[]\n",
    "for e in co:\n",
    "    for i in e.split():\n",
    "        dic3.append(i)\n",
    "text=' '.join(dic2)\n",
    "###for pos\n",
    "doc = nlp(text)\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "show frequencies yes\n",
      "on consiedere les mots vides?yes/no yes\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f4ac5a099631>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mfrequency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mliste4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m'yes'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmotvidesquestion\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mliste2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mfrequency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mliste2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'show patterns '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'yes'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "if input('show frequencies ')=='yes':\n",
    "    motvidesquestion=input('on consiedere les mots vides?yes/no ')\n",
    "    if 'no' in motvidesquestion:\n",
    "        listvide=open('stopwords.txt','r',encoding='utf-8')\n",
    "        lv=listvide.readlines()\n",
    "        dicvide=[]\n",
    "        for e in lv:\n",
    "            dicvide.append(e.replace('\\n','').replace('\\t',''))\n",
    "        liste2=tokenizer(text)\n",
    "        #lenght totale\n",
    "        print(len(liste2),\"lenght of corpus\")\n",
    "        liste4=[e for e in liste2 if e not in dicvide]\n",
    "        #combien de mots vide?\n",
    "        print(len(liste2)-len(liste4),\" stop words\")\n",
    "        #lenght sans mots vides\n",
    "        print(len(liste4),\" -without stop words\")\n",
    "        frequency(liste4)\n",
    "    elif 'yes' in motvidesquestion:\n",
    "        liste2=tokenizer(text)\n",
    "        frequency(liste2)\n",
    "elif input('show patterns ')=='yes':\n",
    "        patterngiver(doc)\n",
    "elif input('show ponctuations stats ')=='yes':\n",
    "    cptinterog=0\n",
    "    for e in dicc:\n",
    "        for i in e:\n",
    "            if '?' in i:\n",
    "                cptinterog+=1\n",
    "    print(cptinterog,\"- ?\")\n",
    "    #combien de phrases exclamatives?\n",
    "    cptex=0\n",
    "    for e in dic3:\n",
    "        for i in e:\n",
    "            if '!' in i:\n",
    "                cptex+=1\n",
    "    print(cptex,\"- !\")\n",
    "    #combien de phrases avec ...?\n",
    "    cptpts=0\n",
    "    for e in dic3:\n",
    "        if '...' in e:\n",
    "            cptpts+=1\n",
    "    print(cptpts,\"- ...\")\n",
    "elif input('show statistique de parties de discours ')=='yes':\n",
    "    posstats(doc)\n",
    "elif input('specificity of the word: ')=='yes':\n",
    "    specificity(text)\n",
    "elif input('les phrases nominales ')=='yes':\n",
    "    phrasesnominales(dic)\n",
    "elif input('longuer des mots ')=='yes':\n",
    "    longuermots(doc)\n",
    "elif input('contexte des mots ')=='yes':\n",
    "    contexte(text,dic3)\n",
    "else:\n",
    "    print('write it correctly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "##gives liste of words poatags and syntax rol\n",
    "for token in doc:\n",
    "    \n",
    "    print(token.text, token.pos_, token.dep_)\n",
    "#print([(word.text, word.pos_) for word in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=text.split('***')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clustering\n",
    "import clustervisualizer\n",
    "import plottingvso2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidfer(sentences):\n",
    "    analyzer = clustering.Clustering(stopwords=False, tfidf=True, stemming=True, nbclusters=5, algo=\"spectral\", dist=\"manhattan\")\n",
    "    dtm, vocab = analyzer.preprocess(sentences)\n",
    "    \n",
    "    print(len(vocab))\n",
    "    print(vocab)\n",
    "    print ('ETAPE 2 - tf-idf')\n",
    "    listeTF=[]\n",
    "\n",
    "    for vecteur in dtm:\n",
    "        res=''\n",
    "        for chaine in vecteur:  \n",
    "            res += str(chaine)+' '\n",
    "        listeTF.append(res)\n",
    "    print ('count sentences :'+str(len(sentences))+' count listeTF :'+str(len(listeTF)))\n",
    "    print ('ETAPE 3- écriture du fichier')\n",
    "    fic_out_vect=open(\"fichier_tf_idf.txt\", 'w')\n",
    "    f=open(\"vocab.txt\",'w')\n",
    "    nb_sentences =len(sentences)\n",
    "    for i in  range(nb_sentences):\n",
    "        fic_out_vect.write(listeTF[i]+'\\n')\n",
    "    dic=[]\n",
    "    for i in dtm:\n",
    "        for i in dtm:\n",
    "            for e in i:\n",
    "                for w in vocab:\n",
    "                    if w not in dic:\n",
    "                        dic.append(w)\n",
    "                        dic.append(str(e))\n",
    "    for e in dic:\n",
    "        f.write(e)\n",
    "        f.write('\\n')\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    df = pd.DataFrame(dtm, columns = vocab)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "373\n",
      "['04h07' '11h09' '150' '17' '2013' '2d' 'abon' 'abond' 'accept'\n",
      " 'accompagn' 'actuel' 'admet' 'affût' 'agor' 'ajust' 'alexandr' 'alor'\n",
      " 'ambassadric' 'analys' 'andersen' 'anna' 'annonc' 'appréhens' 'articl'\n",
      " 'assist' 'astreint' 'attent' 'aujourdhui' 'aur' 'auss' 'aut' 'bambin'\n",
      " 'beaut' 'bel' 'bilan' 'bleus' 'box' 'bureau' 'cach' 'carbon' 'cartabl'\n",
      " 'caus' 'cel' 'cens' 'cest' 'chanson' 'chef' 'cherch' 'chevelur'\n",
      " 'christian' 'citoyen' 'classiqu' 'climat' 'comit' 'comm' 'communaut'\n",
      " 'conclur' 'concret' 'concrétis' 'consider' 'cont' 'contenus' 'contr'\n",
      " 'contradictoir' 'convaincr' 'convent' 'convers' 'cop21' 'coprésident'\n",
      " 'corp' 'cris' 'croir' 'crédibil' 'crédibilis' 'côt' 'cœur' 'dabstract'\n",
      " 'dambit' 'dan' 'danim' 'danois' 'dappréhens' 'derni' 'despoir' 'deuxiem'\n",
      " 'devr' 'dic' 'dilemm' 'dinquiétud' 'diptyqu' 'directeur' 'dirig' 'disney'\n",
      " 'don' 'doubl' 'dress' 'dun' 'débat' 'décid' 'déclench' 'découvert'\n",
      " 'découvr' 'défenseur' 'défin' 'déjà' 'délivr' 'démocrat' 'démêl' 'déploi'\n",
      " 'dépoussier' 'déput' 'désorm' 'détap' 'détermin' 'elsa' 'emmanuel'\n",
      " 'encor' 'end' 'endorm' 'enfant' 'enjeu' 'ex' 'exceptionnel' 'exercic'\n",
      " 'exprim' 'exécut' 'fac' 'fameux' 'faut' 'faveur' 'film' 'fin' 'form'\n",
      " 'franc' 'froid' 'gag' 'gar' 'gen' 'gilet' 'glac' 'gouvern' 'grand'\n",
      " 'grandi' 'graphiqu' 'gros' 'grâc' 'guerin' 'guerr' 'général' 'habil'\n",
      " 'habituel' 'han' 'hulot' 'identifi' 'identitair' 'idé' 'ii' 'impos'\n",
      " 'ined' 'ingrédient' 'inspir' 'invit' 'janvi' 'jaun' 'jou' 'jour' 'jug'\n",
      " 'ki' 'lact' 'laction' 'lambit' 'lattent' 'laurenc' 'lelys' 'lemari'\n",
      " 'lenviron' 'letat' 'lex' 'lexécut' 'lheur' 'lhéroïn' 'liber' 'libr'\n",
      " 'lintérêt' 'lir' 'lissu' 'loin' 'loir' 'longtemp' 'longu' 'lor' 'lrm'\n",
      " 'lutt' 'lécolog' 'lécrivain' 'lédif' 'machin' 'macron' 'magiqu' 'maine'\n",
      " 'majeur' 'major' 'mal' 'mani' 'march' 'matthieu' 'mesur' 'mettr' 'mi'\n",
      " 'ministr' 'mis' 'modern' 'moin' 'moment' 'mond' 'montr' 'moyen' 'mémor'\n",
      " 'mêm' 'naissanc' 'nav' 'neig' 'nest' 'nicol' 'nouvel' 'nov' 'novembr'\n",
      " 'névros' 'objet' 'observent' 'offic' 'once' 'opportun' 'optim' 'orphelin'\n",
      " 'outbrain' 'outil' 'parcour' 'parent' 'parm' 'part' 'particip' 'passent'\n",
      " 'patron' 'pech' 'pein' 'pens' 'personnag' 'peu' 'plan' 'polit' 'pos'\n",
      " 'posit' 'pouvoir' 'premi' 'prendr' 'preuv' 'princess' 'pris' 'problem'\n",
      " 'processus' 'proch' 'profus' 'prostat' 'présent' 'président'\n",
      " 'présidentiel' 'prévus' 'prôn' 'psychanalys' 'publi' 'publiqu' 'puiss'\n",
      " 'pyjam' 'périll' 'quil' 'quinquennat' 'quitt' 'quon' 'quotidien' 'quêt'\n",
      " 'raison' 'rang' 'recet' 'regain' 'regroup' 'rein' 'rendr' 'respons'\n",
      " 'retrac' 'retrouv' 'revien' 'revient' 'revêt' 'rob' 'réchauff' 'républ'\n",
      " 'réserv' 'résoudr' 'résum' 'réuss' 'réussit' 'révolutionnair' 'sajoutent'\n",
      " 'san' 'sant' 'sav' 'secondair' 'ser' 'session' 'sest' 'simpl' 'sisol'\n",
      " 'social' 'sort' 'sponsoris' 'stanisl' 'studio' 'succes' 'suff' 'suscit'\n",
      " 'sœur' 'tank' 'tax' 'temp' 'tendent' 'tent' 'tenu' 'term' 'terr' 'test'\n",
      " 'thierry' 'think' 'tiennent' 'tir' 'titr' 'total' 'trac' 'tradit'\n",
      " 'traduct' 'transit' 'travail' 'traval' 'truc' 'tubian' 'têt' 'univer'\n",
      " 'vant' 'variat' 'ver' 'vess' 'visuel' 'vocat' 'voir' 'vol' 'volet'\n",
      " 'volont' 'votr' 'voyag' 'vérit' 'week' 'xceed' 'xceedpubl' 'yeux'\n",
      " 'écolog' 'écran' 'émerg' 'éniem' 'éprouv' 'épur' 'équilibr' 'évalu'\n",
      " 'évoqu']\n",
      "ETAPE 2 - tf-idf\n",
      "count sentences :2 count listeTF :2\n",
      "ETAPE 3- écriture du fichier\n"
     ]
    }
   ],
   "source": [
    "tfidfer(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

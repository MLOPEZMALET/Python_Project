{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import re\n",
    "from collections import Counter\n",
    "import math\n",
    "import RAKE\n",
    "from rake_nltk import Rake\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "Rk = RAKE.Rake(RAKE.SmartStopList())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.read_csv(\"../train/storyzy_en_train.tsv\", sep=\"\\t\")\n",
    "df_test = pandas.read_csv(\"../test1-full/storyzy_en_test1_full.tsv\", sep=\"\\t\")\n",
    "df = pandas.concat([df, df_test], ignore_index=True)\n",
    "df_fake = df[df[\"type\"] == \"fakeNews\"]\n",
    "df_trusted = df[df[\"type\"] == \"trusted\"]\n",
    "df_satire = df[df[\"type\"] == \"satire\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['texts']  = df['title'].map(str) + df['text']\n",
    "df['words'] = df.texts.apply(lambda doc: re.sub(\"[\\W\\d]\", \" \", doc.lower().strip()).split())\n",
    "df['words_stem'] = df.words.apply(lambda word: [stemmer.stem(w) for w in word])\n",
    "df['words_lem'] = df.words.apply(lambda word: [lemmatiser.lemmatize(w) for w in word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracted keywords with rake\n",
    "\n",
    "df[\"keyword_rake\"]= df.texts.apply(lambda doc : [w[0] for w in Rk.run(doc.lower().strip(), maxWords = 1)])\n",
    "df[\"keyword_rake_lem\"]= df.words_lem.apply(lambda doc : [w[0] for w in Rk.run(\" \".join(u for u in doc), maxWords = 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####extracted keywords with TFIDF + stemming\n",
    "corpus_size = len(df)\n",
    "\n",
    "#let's calculate the word frequencies for each document (Bag of words)\n",
    "df['frequencies_stem'] = df.words_stem.apply(lambda words_stem: Counter(words_stem))\n",
    "\n",
    "#cool, now we can calculate TF, the log+1 of the frequency of each word\n",
    "df['log_frequencies_stem'] = df.frequencies_stem.apply(lambda d: dict([(k,math.log(v) + 1) for k, v in d.items()]))\n",
    "\n",
    "#now let's build up a lookup list of document frequencies\n",
    "#first we build a vocabulary for our corpus(set of unique words)\n",
    "corpus_vocab = set([word for words in df.words_stem for word in words])\n",
    "\n",
    "#now use the vocabulary to find the document frequency for each word\n",
    "df_2 = lambda word: len(df[df.words_stem.apply(lambda w: word in w)])\n",
    "corpus_vocab_dfs = dict([(word,math.log(corpus_size / df_2(word))) for word in corpus_vocab])\n",
    "\n",
    "\n",
    "#phew! no let's put it all together. let's calculate tf*idf for each term\n",
    "tfidf = lambda tfs: dict([(k,v * corpus_vocab_dfs[k]) for k, v  in tfs.items()])\n",
    "df['tfidf_stem'] = df.log_frequencies_stem.apply(tfidf)\n",
    "\n",
    "#finally we can grab the top 5 weighted terms to get keywords for each document\n",
    "sorted(df.tfidf_stem[0], key=df.tfidf_stem[0].get, reverse=True)[0:50]\n",
    "df['keywords_stem'] = df.tfidf_stem.apply(lambda t: sorted(t, key=t.get, reverse=True)[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####extracted keywords with TFIDF + lemmatisation\n",
    "corpus_size = len(df)\n",
    "\n",
    "#let's calculate the word frequencies for each document (Bag of words)\n",
    "df['frequencies_lem'] = df.words_lem.apply(lambda words_lem: Counter(words_lem))\n",
    "\n",
    "#cool, now we can calculate TF, the log+1 of the frequency of each word\n",
    "df['log_frequencies_lem'] = df.frequencies_lem.apply(lambda d: dict([(k,math.log(v) + 1) for k, v in d.items()]))\n",
    "\n",
    "#now let's build up a lookup list of document frequencies\n",
    "#first we build a vocabulary for our corpus(set of unique words)\n",
    "corpus_vocab = set([word for words in df.words_lem for word in words])\n",
    "\n",
    "#now use the vocabulary to find the document frequency for each word\n",
    "df_2 = lambda word: len(df[df.words_lem.apply(lambda w: word in w)])\n",
    "corpus_vocab_dfs = dict([(word,math.log(corpus_size / df_2(word))) for word in corpus_vocab])\n",
    "\n",
    "\n",
    "#phew! no let's put it all together. let's calculate tf*idf for each term\n",
    "tfidf = lambda tfs: dict([(k,v * corpus_vocab_dfs[k]) for k, v  in tfs.items()])\n",
    "df['tfidf_lem'] = df.log_frequencies_lem.apply(tfidf)\n",
    "\n",
    "#finally we can grab the top 5 weighted terms to get keywords for each document\n",
    "sorted(df.tfidf_lem[0], key=df.tfidf_lem[0].get, reverse=True)[0:50]\n",
    "df['keywords_lem'] = df.tfidf_lem.apply(lambda t: sorted(t, key=t.get, reverse=True)[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####extracted keywords with TFIDF\n",
    "corpus_size = len(df)\n",
    "\n",
    "#let's calculate the word frequencies for each document (Bag of words)\n",
    "df['frequencies'] = df.words.apply(lambda words: Counter(words))\n",
    "\n",
    "#cool, now we can calculate TF, the log+1 of the frequency of each word\n",
    "df['log_frequencies'] = df.frequencies.apply(lambda d: dict([(k,math.log(v) + 1) for k, v in d.items()]))\n",
    "\n",
    "#now let's build up a lookup list of document frequencies\n",
    "#first we build a vocabulary for our corpus(set of unique words)\n",
    "corpus_vocab = set([word for words in df.words for word in words])\n",
    "\n",
    "#now use the vocabulary to find the document frequency for each word\n",
    "df_2 = lambda word: len(df[df.words.apply(lambda w: word in w)])\n",
    "corpus_vocab_dfs = dict([(word,math.log(corpus_size / df_2(word))) for word in corpus_vocab])\n",
    "\n",
    "\n",
    "#phew! no let's put it all together. let's calculate tf*idf for each term\n",
    "tfidf = lambda tfs: dict([(k,v * corpus_vocab_dfs[k]) for k, v  in tfs.items()])\n",
    "df['tfidf'] = df.log_frequencies.apply(tfidf)\n",
    "\n",
    "#finally we can grab the top 5 weighted terms to get keywords for each document\n",
    "sorted(df.tfidf_lem[0], key=df.tfidf_lem[0].get, reverse=True)[0:50]\n",
    "df['keywords'] = df.tfidf.apply(lambda t: sorted(t, key=t.get, reverse=True)[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the file\n",
    "df = df.drop(columns=['log_frequencies', 'frequencies', 'tfidf', 'frequencies_lem', 'log_frequencies_lem', 'tfidf_lem',\n",
    "       'frequencies_stem', 'log_frequencies_stem', 'tfidf_stem'])\n",
    "df.to_csv(\"../train/last_data_tfidf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nuage de mots\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "\n",
    "df = pd.read_csv(\"../train/last_data_tfidf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keywords_fake = []\n",
    "all_keywords_trusted = []\n",
    "all_keywords_satire = []\n",
    "for keyword in df.loc[df[\"type\"] == \"fakeNews\", \"keywords\"]:\n",
    "    all_keywords_fake += keyword[2:(len(keyword)-2)].split(\"', '\")\n",
    "\n",
    "for keyword in df.loc[df[\"type\"] == \"trusted\", \"keywords\"]:\n",
    "    all_keywords_trusted += keyword[2:(len(keyword)-2)].split(\"', '\")\n",
    "    \n",
    "for keyword in df.loc[df[\"type\"] == \"satire\", \"keywords\"]:\n",
    "    all_keywords_satire += keyword[2:(len(keyword)-2)].split(\"', '\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_fake = Counter(all_keywords_fake).most_common()\n",
    "count_trusted = Counter(all_keywords_trusted).most_common()\n",
    "count_satire = Counter(all_keywords_satire).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_fake = {}\n",
    "for k, v in count_fake:\n",
    "    dict_fake[k] = int(v) \n",
    "    \n",
    "    \n",
    "dict_trusted = {}\n",
    "for k, v in count_trusted:\n",
    "    dict_trusted[k] = int(v) \n",
    "    \n",
    "dict_satire = {}\n",
    "for k, v in count_satire:\n",
    "    dict_satire[k] = int(v) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_fake = WordCloud().generate_from_frequencies(dict_fake)\n",
    "wordcloud_trusted = WordCloud().generate_from_frequencies(dict_trusted)\n",
    "wordcloud_satire = WordCloud().generate_from_frequencies(dict_satire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "plt.imshow(wordcloud_fake, interpolation='bilinear')\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "plt.imshow(wordcloud_trusted, interpolation='bilinear')\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "plt.imshow(wordcloud_satire, interpolation='bilinear')\n",
    "plt.axis(\"off\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
